// Generated by the Tensor Algebra Compiler (tensor-compiler.org)
// ./taco "A(i,j) = B(i,k) * C(k,j)" -d=B:100,100 -d=C:100,100 -d=A:100,100 -f=B:dd -f=C:dd -f=A:dd -cuda -write-source=tmp.c
#ifndef TACO_C_HEADERS
#define TACO_C_HEADERS
#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>
#include <math.h>
#include <thrust/complex.h>
#define TACO_MIN(_a,_b) ((_a) < (_b) ? (_a) : (_b))
#define TACO_MAX(_a,_b) ((_a) > (_b) ? (_a) : (_b))
#define TACO_DEREF(_a) (((___context___*)(*__ctx__))->_a)
#ifndef TACO_TENSOR_T_DEFINED
#define TACO_TENSOR_T_DEFINED
typedef enum { taco_mode_dense, taco_mode_sparse } taco_mode_t;
typedef struct {
  int32_t      order;         // tensor order (number of modes)
  int32_t*     dimensions;    // tensor dimensions
  int32_t      csize;         // component size
  int32_t*     mode_ordering; // mode storage ordering
  taco_mode_t* mode_types;    // mode storage types
  uint8_t***   indices;       // tensor index data (per mode)
  uint8_t*     vals;          // tensor values
  int32_t      vals_size;     // values array size
} taco_tensor_t;
#endif
#endif


#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
{
  if (code != cudaSuccess)
  {
    fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
    if (abort) exit(code);
  }
}

__global__
void computeDeviceKernel0(taco_tensor_t *A, taco_tensor_t *B, taco_tensor_t *C){
  int A2_dimension = (int)(A->dimensions[1]);
  double* __restrict__ A_vals = (double*)(A->vals);
  int B1_dimension = (int)(B->dimensions[0]);
  int B2_dimension = (int)(B->dimensions[1]);
  double* __restrict__ B_vals = (double*)(B->vals);
  int C1_dimension = (int)(C->dimensions[0]);
  int C2_dimension = (int)(C->dimensions[1]);
  double* __restrict__ C_vals = (double*)(C->vals);

  int32_t i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i >= B1_dimension) {
    return;
  }

  for (int32_t k = 0; k < C1_dimension; k++) {
    int32_t pB2 = i * B2_dimension + k;
    for (int32_t j = 0; j < C2_dimension; j++) {
      int32_t pA2 = i * A2_dimension + j;
      int32_t pC2 = k * C2_dimension + j;
      A_vals[pA2] = A_vals[pA2] + B_vals[pB2] * C_vals[pC2];
    }
  }
}

int compute(taco_tensor_t *A, taco_tensor_t *B, taco_tensor_t *C) {
  int A1_dimension = (int)(A->dimensions[0]);
  int A2_dimension = (int)(A->dimensions[1]);
  double* __restrict__ A_vals = (double*)(A->vals);
  int B1_dimension = (int)(B->dimensions[0]);

  #pragma omp parallel for
  for (int32_t pA = 0; pA < (A1_dimension * A2_dimension); pA++) {
    A_vals[pA] = 0.0;
  }

  computeDeviceKernel0<<<(B1_dimension + 255) / 256, 256>>>(A, B, C);
  cudaDeviceSynchronize();
  return 0;
}

int assemble(taco_tensor_t *A, taco_tensor_t *B, taco_tensor_t *C) {
  int A1_dimension = (int)(A->dimensions[0]);
  int A2_dimension = (int)(A->dimensions[1]);
  double* __restrict__ A_vals = (double*)(A->vals);

  gpuErrchk(cudaMallocManaged((void**)&A_vals, sizeof(double) * A1_dimension * A2_dimension));

  A->vals = (uint8_t*)A_vals;
  return 0;
}

__global__
void evaluateDeviceKernel0(taco_tensor_t *A, taco_tensor_t *B, taco_tensor_t *C){
  int A2_dimension = (int)(A->dimensions[1]);
  double* __restrict__ A_vals = (double*)(A->vals);
  int B1_dimension = (int)(B->dimensions[0]);
  int B2_dimension = (int)(B->dimensions[1]);
  double* __restrict__ B_vals = (double*)(B->vals);
  int C1_dimension = (int)(C->dimensions[0]);
  int C2_dimension = (int)(C->dimensions[1]);
  double* __restrict__ C_vals = (double*)(C->vals);

  int32_t i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i >= B1_dimension) {
    return;
  }

  for (int32_t k = 0; k < C1_dimension; k++) {
    int32_t pB2 = i * B2_dimension + k;
    for (int32_t j = 0; j < C2_dimension; j++) {
      int32_t pA2 = i * A2_dimension + j;
      int32_t pC2 = k * C2_dimension + j;
      A_vals[pA2] = A_vals[pA2] + B_vals[pB2] * C_vals[pC2];
    }
  }

}

int evaluate(taco_tensor_t *A, taco_tensor_t *B, taco_tensor_t *C) {
  int A1_dimension = (int)(A->dimensions[0]);
  int A2_dimension = (int)(A->dimensions[1]);
  double* __restrict__ A_vals = (double*)(A->vals);
  int B1_dimension = (int)(B->dimensions[0]);

  int32_t A_capacity = A1_dimension * A2_dimension;
  gpuErrchk(cudaMallocManaged((void**)&A_vals, sizeof(double) * A_capacity));

  #pragma omp parallel for
  for (int32_t pA = 0; pA < A_capacity; pA++) {
    A_vals[pA] = 0.0;
  }

  evaluateDeviceKernel0<<<(B1_dimension + 255) / 256, 256>>>(A, B, C);
  cudaDeviceSynchronize();

  A->vals = (uint8_t*)A_vals;
  return 0;
}
